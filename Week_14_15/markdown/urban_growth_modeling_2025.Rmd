---
title: "CPLN 6750 - Urban Growth Modeling, 2025"
author: "Michael Fichman, Ken Steif & Jenna Epstein"
date: "5/2/2025"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>


_There is a very cool lineage for this class module. It is inspired by the California Urban Futures Model, developed by Prof. Emeritus John Landis as what he called a "reduced form" model. Prof Landis taught his version of this methodology when I was his student. Years later, I team-taught this class with the late Prof. Ken Steif, who was another student of John's. We adapted the approach to be done in R, and use more explicitly spatial features. Ken wrote about it in an unpublished chapter from his book "Public Policy Analytics". I include some of Ken's unpublished writing here in its original form._

_This workflow will form the backbone of your fifth and final assignment for CPLN 6750. You will build a growth model based on a logistic regression that predicts the probability of land cover change across space over a specified time interval. Land cover change is hypothesized here to be a function of a) Existing land cover, b) The location of infrastructure and c) Demographic and economic spatial variables._

_For the first time in this class, you will have the option of conducting this workflow entirely in R._

_Thanks very much to Jenna Epstein for editing this material._

_Michael Fichman, March, 2025_


# 1. Introduction

*"Patterns of regional development emerge from the autonomous decisions of many different agents - including developers, real estate buyers and tenants as well as planners and regulators. Each of these groups optimize for a different set of bottom lines. Developers seek profit, buying land at a low price, improving on that land, and selling it at a premium. Households and firms consume housing and commercial real estate balancing price constraints with access to amenities and customers, respectively.*

*"Planners regulate development by trading-off economic growth with the mitigation of negative externalities toward the goal of economic and environmental sustainability. Houston, the focus of this chapter, is a case study in managing these trade-offs. A sprawling Metropolitan area larger than the state of New Jersey, Houston’s laissez-faire approach to planning was exploited in the wake of Hurricane Harvey which ultimately cost the region $125 billion. Out of concern for either safety, economic necessity or both, climate change is forcing cities like Houston to reconsider the role of land use planning.*

*"Land use planning that is both economically productive and sustainable requires both supply and demand-side insights. Specifically, the Planner must understand future demand for development and how that demand contrasts with the supply of environmentally sensitive land. The goal of this chapter is to model this interplay....*

*"At the center of our model is a hypothesis that development demand must in part, be a function of the pattern of existing development. Development occurs where the market believes a higher and better use may bring an investment return. In the case of sprawling region like Houston, assuming the requisite demand, there is a clear return on investment for converting farmland to suburban housing.*

*"The traditional ‘bid-rent’ economic model of development posits that development demand is a function of accessibility. This model works well in cities where centralized locations offer the most accessibility. However, it assumes that all consumers of land share the same preferences for central city access. While urban land is valuable, contemporary urbanism in regions like Houston show us that suburban locations can be quite desirable as well.*

*"Why is that? Hinterland locations do not offer direct access to jobs and cultural amenities. Instead, residents trade-off accessibility for larger lots and bigger homes; as well as a bundle of public services like school quality. Developers are attracted to suburban and exurban locations because of cheap land on ‘greenfield’ sites like farms and open space.*

*"The demand for greenfield development can vary substantially depending on the existing spatial configuration of development. If accessibility to central locations was the only underlying consideration, developers would sprawl directly out to the periphery... As a space/time process, this would look much like spilled milk, emanating from a central point outward across the kitchen table.*

*"Another option for developers is to move beyond the periphery onto greenfield sites that are cheaper because they are even less accessible. In this case, the space/time process looks small ‘patches’ of new development dotting the landscape and “leapfrogging” from one greenfield to the next. The economic incentive is to always develop beyond the periphery, where land is cheapest. There are some real costs to this model however. For one, when development is so diffuse, it is more burdensome to efficiently deploy infrastructure like roads, sewers and electicity. Second, leapfrog development fragments natural areas reducing biodiversity and stressing the natural habitat of species that need continuous open space to thrive.*

*"In Houston, as in many sprawling regions of the U.S. the economic incentives that underlie sprawl likely encourage both the accessibility and leapfrog models of development. For our purposes however, features must be created to associate these patterns with development. Without them, the model may lack the appropriate spatial experience on which to forecast growth.*

*"To keep it simple, we develop features associated with accessibility-based patterns. In reality, the analyst should develop a series of applicable features and test which best associate with the outcome of interest. The problem becomes infinitely more difficult when one realizes that sprawl patterns may differ throughout the study area - if for instance, land use restrictions varied by county. Below we estimate models using logistic regression, but higher level machine learning algorithms, most notably, Random Forest, are more adept at dealing with non-linearities across space.*

*- Ken Steif, 2019*

We want to create a model that is accurate, but also useful. How can a forecast inform our planning intelligence? Can we understand where sensitive lands might be at risk? Can we understand where interventions - such as upzoning - might allow us to accommodate likely demand while minimizing sprawl? Can we estimate the impacts of new infrastructure before it's built? We will generate a basic model that will let us analyze alternative futures and think about interventions to shape that future.

# 1.2. The Workflow

The goal of this analysis is to predict the likelihood of land cover change in the Houston MSA from *undeveloped* to *developed* over a certain time interval based on demographic, infrastructural, and environmental information from a baseline year roughly ten years previous (*t1*). We train a binomial logit model using information from year *t1* and project land cover changes over a roughly ten year interval for year *t2*. By feeding identical independent variables for year *t2* to your model, you can forecast a land use pattern for a future year *t3*.

While we have used logistic regression to model spatial phenomena like flooding before, this workflow is different in that we wrangle raster data in R rather than doing it in ArcGIS. **You ABSOLUTELY do not need to master the code in detail, but rather, you should understand what it does operationally, and think about how you can plug in new data from a different time window, in a different geographic context.**

Land cover rasters are not available for all years, so the analysis shown here uses an eight year window - 2011 (*t1*) to 2019 (*t2*) to generate forecasts for 2027 (*t3*).

**The analytical workflow is as follows:**

1. **Import land cover rasters for the study area MSA in time periods t1 and t2**.

2. **Resample** the rasters to reduce the resolution to something that is manageable for processing on your computer.

3. **Reclassify** these rasters into *developed* and *undeveloped* land cover categories. Use map algebra to identify areas that were undeveloped in base year *t1* that were developed in *t2* - reclassify these as `1` - our outcome of interest.

4. **Create a "fishnet" grid** for our study area (grid cell polygons), and summarize our raster information into these fishnet cells. Each cell is now a row, with columns for our dependent variable `development_change` (conversion to developed) and our independent variables, which we will engineer in subsequent steps.

5.**Engineer variables** that you hypothesize are **independent predictors of the likelihood a grid cell converts from undeveloped to developed between *t1* and *t2*. **

The variables used in this workflow include distance to the nearest highway (using `tigris` data on highways), the *spatial lag of development* e.g. the nearness to the closest developed fishnet cell, and census variables including population density.

Build identical variables for *t1* and *t2* - you will need the *t2* data to make your projections for *t3*.

6. **Conduct exploratory analysis** to see what independent variables might be correlated with `development_change` and to examine the data set for spatial patterns that might provide clues about feature engineering.

7. Split your data into training and testing sets and **train a binomial logistic regression model** using *t1* data that can predict `development_change`.

8. **Validate your model** by examining errors in your test set using a confusion matrix, ROC curve, plots and maps.

9. **Optimize the model for accuracy and generalizability** through feature engineering, feature selection, and changes to your classification threshold.

10. **Output a forecast** of future development using your final model and variables from *t2*

11. **Interpret and analyze your forecast** using your domain expertise in planning. This includes an analysis of projected development on sensitive lands and a detail of forecasts by county. This is a precursor to any planning or policy action regarding infrastructure, zoning, preservation, incentives or other tools.

# 1.3. Alternative Workflow Notes for ArcGIS

This document functions as a template for a class assignment in CPLN 6750 - different student teams might choose to build spatial features with ArcGIS. The workflow would function in the same way as described in the previous section - loading in two time periods of land cover data, reclassifying it, identifying change, and then summarizing to a fishnet. You can calculate distance to highways, spatial lag to nearest developed cell, and other spatial variables in ArcGIS.

However, you will need to bring your data from *t1* and *t2* into R in fishnet form in order to run your logistic regression models. In this case, bring your data into R in step 10 "Creating Our Data Set for Modeling".

# 1.4. Setup - Loading Libraries and Functions

Below we load the libraries needed for the analysis. A set of palette colors are also specified.

The packages are as follows

- tidyverse - The main data wrangling and visualization packages including dplyr and ggplot2

- sf - The core R-based vector GIS package

- raster - A raster GIS package - this package doesn't always use the "tidy" syntax we are used to. `terra` is the main competitor for raster work in R - if you don't like `raster`, try it out.

- kableExtra - a package that makes nice readable tables in R Markdown

- tidycensus - a package for calling census data from the US Census API

- tigris - a library for calling US Census Bureau vector GIS data

- FNN - a spatial point-pattern analysis package that we use for K-nearest neighbor

- caret - a package used for applied predictive modeling

- yardstick - a package for summarizing model performance

- plotROC - a tool to plot ROC curves, a diagnostic for logistic regression

- ggrepel - graphics package to handle labels in ggplot2

- pROC - a package to analyze ROC curves

- grid - graphics package to display multiple plots in one visualization

- gridExtra - a package used to make small multiple arrangements of ggplots

- viridis - A library with nice color ramps for our charts and maps.

- igraph - A package for network analysis

- mapview - A quick web mapping package that can handle sf and raster data

- FedData - A package for accessing federal satellite imagery via API

```{r load_packages, message=FALSE, warning=FALSE, results = "hide"}
library(tidyverse)
library(sf)
library(raster)
library(kableExtra)
library(tidycensus)
library(tigris)
library(FNN)
library(caret)
library(yardstick)
library(plotROC) 
library(ggrepel)
library(pROC)
library(grid)
library(gridExtra)
library(viridis)
library(igraph)
library(mapview)
library(FedData)

palette2 <- c("#41b6c4","#253494")
palette4 <- c("#a1dab4","#41b6c4","#2c7fb8","#253494")
palette5 <- c("#ffffcc","#a1dab4","#41b6c4","#2c7fb8","#253494")
palette10 <- c("#f7fcf0","#e0f3db","#ccebc5","#a8ddb5","#7bccc4",
               "#4eb3d3","#2b8cbe","#0868ac","#084081","#f7fcf0")
```

We will also load several functions that are custom built for use in this workflow: 

`quintileBreaks` takes a dataframe and a column name and outputs the quintile breaks, helping shorten the below `ggplot` calls.

`xyC` quickly plots sf polygons by converting a fishnet into a dataframe of grid cell centroid coordinates that can be plotted using `geom_point`.

`rast` quickly converts raster objects in R into data frames for quick plotting in `ggplot`

`nn_function` measures the distance from one `sf` point object to its k-nearest-neighbors from another `sf` point object

`aggregateRaster`loops through a list of rasters, converts the each raster to points, filters only points that have value of `1` (ie. is the _ith_ land cover type), and then aggregates to the fishnet.

The following code block loads these functions:

```{r, warning = FALSE, message = FALSE}
#this function converts a column in to quintiles. It is used for mapping.
quintileBreaks <- function(df,variable) {
    as.character(quantile(df[[variable]],
                          c(.01,.2,.4,.6,.8),na.rm=T))
}

#This function can be used to convert a polygon sf to centroids xy coords.
xyC <- function(aPolygonSF) {
  as.data.frame(
    cbind(x=st_coordinates(st_centroid(aPolygonSF))[,1],
          y=st_coordinates(st_centroid(aPolygonSF))[,2]))
} 

#this function convert a raster to a data frame so it can be plotted in ggplot
rast <- function(inRaster) {
  data.frame(
    xyFromCell(inRaster, 1:ncell(inRaster)), 
    value = getValues(inRaster)) }

# knn distance function

nn_function <- function(measureFrom,measureTo,k) {
  #convert the sf layers to matrices
  measureFrom_Matrix <-
    as.matrix(measureFrom)
  measureTo_Matrix <-
    as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
    output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}

aggregateRaster <- function(inputRasterList, theFishnet) {
  #create an empty fishnet with the same dimensions as the input fishnet
  theseFishnets <- theFishnet %>% dplyr::select()
  #for each raster in the raster list
  for (i in inputRasterList) {
  #create a variable name corresponding to the ith raster
  varName <- names(i)
  #convert raster to points as an sf
    thesePoints <-
      rasterToPoints(i) %>%
      as.data.frame() %>%
      st_as_sf(coords = c("x", "y"), crs = st_crs(theFishnet)) %>%
      filter(.[[1]] == 1)
  #aggregate to the fishnet
    thisFishnet <-
      aggregate(thesePoints, theFishnet, length) %>%
      mutate(!!varName := ifelse(is.na(.[[1]]),0,1))
  #add to the larger fishnet
    theseFishnets <- cbind(theseFishnets,thisFishnet)
  }
  #output all aggregates as one large fishnet
   return(theseFishnets)
  }
```

## 1.5. A Note About Census Data

As of this writing, in April of 2025, there has been recent inconsistent service with the US Census API's tigerline geometries. This has affected the `tidycensus` and `tigris` packages. If you experience issues using tidycensus or tigris, refer to the "issues" section of the respective packages github pages to see if your error is being discussed and troublehot.

If you get the following error, this means the API is down:

`The source could be corrupt or not supported. See `st_drivers()` for a list of supported formats.`

The way to solve this is to uninstall tidycensus and tigris, and install a "patched" version of `tigris` direct from developer Kyle Walker's github repo using the `devtools` package. Then you reinstall tigris first, and tidycensus second. The patched version of tigris will get the data from the Census FTP server instead of the API. [There is a quick code vignette I've created to walk you through this](https://github.com/mafichman/CPLN_675/blob/main/misc/tigris_patch.R).

# 2. Loading LC and MSA data

## 2.1. Load the study area MSA

Our study area is the Houston MSA. We can `st_read` a spatial data object directly from Github, where we have our class data hosted.

We `st_transform` it to the 2278 coordinate reference system, which is appropriate for Houston and has a linear unit in feet.

For your assignment, you will want to load in data using `tigris` or `tidycensus` for your study area of choice in this step. You might do this by loading the counties that comprise your area, and using `st_union` to turn them all into one shape.

```{r}
houstonMSA <- 
  st_read("https://raw.githubusercontent.com/mafichman/CPLN_675/main/Week_14_15/data/UGB_Chapter_data/houston_MSA.geojson") %>%
  st_transform(2278)
```
## 2.2. Load Land Cover for t1 and t2

We can use a direct API call to import raster data of interest directly into R.

The `get_nlcd` function from the package `FedData` can be used to fetch National Land Cover Database rasters from [www.mrlc.gov](www.mrlc.gov) and then crop it to a `template` that is a `sf` shape that you specify as the boundary for your call. For our template, we will use the `houstonMSA`.

There aren't NLDD available for every year, so we will use 2011 and 2019 data as our *t1* and *t2*, respectively. 

This code block uses the `raster` function creates a data object of `Formal class RasterLayer`

```{r}

lc_2011 <- get_nlcd(
    template = houstonMSA,
    label = "lc_2011",
    year = 2011
  ) %>%
  raster(.)

```



```{r}

lc_2019 <- get_nlcd(
    template = houstonMSA,
    label = "lc_2019",
    year = 2019
  ) %>%
  raster(.)

```

Let's look at some info about these rasters. By typing the name of our data object(s) into the console, we can see what the classes are, their ID numbers, and the fact that they are 30m grid cells.

Recall that the NLCD has numerical classifications for land cover types.

```{r}
lc_2019
```

# 3. Detecting Land Cover Conversion

We have rasters representing land cover in `t1` and `t2` - we want to detect areas where land converted from undeveloped to developed - those cells represent the "1" in the logit model we will develop, and we want to train a model to estimate the likelihood any given cell will be a "1".


## 3.1. Resampling Rasters

We want to resample our rasters to make the resolution much coarser (e.g make the cells much larger). At 30m, trust me, they will crush our laptops when we try to process stuff.

Let's resample them to be 30x coarser resolution. You will make your own resampling deliberation when you do this analysis yourself for your homework. The resolution depends a lot on your study area size, but standard laptop computers are probably best suited to handle thousands or low-tens-of-thousands cells as we build our fishnets and start mapping and modeling.

The `aggregate` function from the `raster` package is used for resampling. We create two new `RasterLayer` data sets with the appendage `_rs` in their name to denote that they are resampled.

Specifying `fun = "modal"` is most appropriate for categorical data like land cover, extracting the modal value of the cells that are being aggregated together. (You can use means, maxima, and other stats for this argument too).

```{r}
lc_2011_rs <- aggregate(lc_2011, fact = 30, fun = "modal")

lc_2019_rs <- aggregate(lc_2019, fact = 30, fun = "modal")

```

## 3.2. Reclassifying As Developed and Undeveloped

In order to figure out which cells are developed or undeveloped, and subsequently identify the ones that were developed between *t1* and *t2*, we need to reclassify our land cover data to consist of 1 and 0 observations (e.g. 1 is the developed classes 13-24, 0 is everything else). Check section 2.2 if you want to see which codes correspond to which numbers in the NLCD.

We create a matrix called `reclassMatrix` with reclassification instructions for the `reclassify` function in the `raster` package.

```{r}
reclassMatrix <- 
  matrix(c(
    0,12,0,
    12,24,1,
    24,Inf,0),
  ncol=3, byrow=T)
```



```{r, warning = FALSE, message = FALSE}
developed_2011 <- 
  reclassify(lc_2011_rs, reclassMatrix)

developed_2019 <- 
  reclassify(lc_2019_rs, reclassMatrix)

```


We can do some map algebra to find the places where land cover changed. 

Notice we can do map algebra really simply with the `raster` package by merely writing an equation. This is similar to how we do it in ArcGIS.


```{r, warning = FALSE, message = FALSE}

development_change <- developed_2011 + developed_2019

```

Let's see a quick histogram of the values - these should range from 0 (undeveloped in 2001, undeveloped in 2011), 1 (undeveloped in 2001, developed in 2011 (presuming nothing went from developed to undeveloped)), and 2 (developed in both periods). 

The 1's represent the change.

```{r, warning = FALSE, message = FALSE}
hist(development_change)
```

We can turn any of the 0's and 2's to NA, leaving only 1's and NA.

```{r, warning = FALSE, message = FALSE}
development_change[development_change != 1] <- NA
```

## 3.3. Examining Land Cover Change

Let's use Mapview to take a look at the `development_change` raster (yes you can make a quick webmap of these rasters - easy!)

```{r}
mapView(development_change)

```

# 4. Creating the Fishnet

We will do our modeling in a "fishnet" - a vector grid which is uniform and raster-like. Each cell has a row in a table - we will need a data frame object to feed to our modeling workflow. We will create our fishnet at the same resolution (e.g. cell size) and CRS as our raster data

The `st_make_grid` function can do this for us by making a fishnet called `houstonMSA_fishnet` at the scale of our `houstonMSA` - our study area. Note the following things that we do in this function.

We `st_transform` the `houstonMSA` to the crs of `development_change` - that's the basis for our grid.

We set `cellsize` argument the `res` (aka resolution) of `development_change`.

Use the `st_sf` function to ensure that our data are in `sf` format (the st_make_grid function returns a different kind of spatial object by devault)

We "clip" the grid to the `houstonMSA` using `st_intersection` - making sure that the MSA data contain only the geometry, projected in the same crs as `development_change`.

And then `mutate` a column in our data set called `uniqueID` - we will need to have a unique ID for each row in our data in case we need to make some tabular joins.

```{r, warning = FALSE, message = FALSE}
houstonMSA_fishnet <- 
  st_make_grid(houstonMSA %>%
                     st_transform(crs(development_change)),
                 cellsize = res(development_change)[1], 
                 square = TRUE) %>% 
  st_sf() %>% 
  st_intersection(., houstonMSA %>%
                    dplyr::select(geometry) %>%
                         st_transform(crs(development_change))) %>%
  mutate(uniqueID = rownames(.))
```

# 5. Join Raster Data to the Fishnet

How do we get all of our raster data into the fishnet? How do we get our development change to show up as a 1/0 variable in a given column? In our models we will want to parameterize things like existing land use, or distance to the nearest highway - how do we create columns for those? The simple answer - t we convert our rasters to points, and then spatially join them to our fishnet.

## 5.1. Aggregating LC Change to fishnet

Let's do a single aggregation below to demonstrate the workflow. You can use this workflow to take any kind of raster data you'd like and coerce it to the fishnet. Later we will demonstrate a batch conversion workflow you can use to get all of your land cover rasters in to a fishnet as a group.

The function `rasterToPoints` (from the `raster` package) turns our raster into a centroid with an `x` and `y` coordinate from its native projection. We can turn this into an `sf` object using `st_as_sf` and specifying the crs as that of `houstonMSA_fishnet`.

Then, we create a fishnet called (creatively) `fishnet` - this will be the core grid we use to build our modeling variables. 

We use the `aggregate` function from the `sf` pacakge to summarize the `changePoints` - note that we use the `sum` argument for the `FUN` argument (e.g. "function") - you could use a min, max, mean etc., in this argument. This creates an sf object that has two columns - `layer` - which has values that correspond to those of the raster.

If you were to run the following code - `aggregate(changePoints, houstonMSA_fishnet, FUN=sum) %>% group_by(layer) %>% tally()` you would see a summary of that column. When we run our aggregate function, we get 1's and NAs.

We pipe in a `mutate` command to specify that there is a new column in `fishnet` we'd like to call `development_change`. We populate it with values from an if_else statement - if layer is NA, make it a 0, if it's not, make it a 1. We make sure this is a factor variable.

Then we use a select statement to remove the column `layer`.

```{r}
changePoints <-
  rasterToPoints(development_change) %>%
  as.data.frame() %>%
  st_as_sf(coords = c("x", "y"), 
           crs = st_crs(houstonMSA_fishnet))

fishnet <- 
  aggregate(changePoints, 
            houstonMSA_fishnet, 
            FUN=sum) %>%
  mutate(development_change = ifelse(is.na(layer) == TRUE , 0, 1),
         development_change = as.factor(development_change)) %>%
  dplyr::select(-layer)

```


How does it look?

Here we make a plot with a little custom function inside our geom_point data - `xyC` turns fishnets into centroids so we can plot them much more quickly than polygons. Fun trick.

```{r}
ggplot() +
  geom_sf(data=houstonMSA %>% 
            st_transform(crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)$x, 
                 y=xyC(fishnet)$y, 
                 colour=development_change)) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name = "") +
  labs(title = "Land Cover Development Change", subtitle = "As fishnet centroids") +
  theme_void()
```


## 5.2. Aggregating Batch Land Cover Data to the Fishnet

OK - we will need to reclassify our get all of our land cover data from `t1` and `t2` into a fishnet environment.  Doing that one raster at a time would be very tedious. I'm providing you with a custom function that can help you aggregate large lists of rasters, called `aggregateRaster`

### 5.2.1. Reclassifying land cover

We are going to recode our NLCD data into a few reduced categories to be used in our analysis. Here is the classification logic: 

| Old_Classification             | New_Classification                                  |
|--------------------------------|-----------------------------------------------------|
| Open Space as well as Low, Medium and High Intensity Development | Developed |
| Deciduous, Evergreen, and Mixed Forest |  Forest |
| Pasture/Hay and Cultivated Crops | Farm |
| Woody and Emergent Herbaceous Wetlands | Woodlands |
| Barren Land, Dwarf Scrub, and Grassland/Herbaceous | Other Undeveloped |
| Water | Water |


In the code block below, we create new rasters, descriptively called `developed_2011`, `forest_2011` etc , according to the NLCD codes that correspond to our classification scheme. 

```{r, warning = FALSE, message = FALSE}

developed_2011 <- lc_2011_rs  %in% c(21, 22, 23, 24)
forest_2011 <- lc_2011_rs %in% c(41, 42, 42)
farm_2011 <- lc_2011_rs %in% c(81, 82)
wetlands_2011 <- lc_2011_rs %in% c(90, 95) 
otherUndeveloped_2011 <- lc_2011_rs %in% c(52, 71, 31)
water_2011 <- lc_2011_rs == 11

developed_2019 <- lc_2019_rs %in% c(21, 22, 23, 24)
forest_2019 <- lc_2019_rs %in% c(41, 42, 42)
farm_2019 <- lc_2019_rs %in% c(81, 82)
wetlands_2019 <- lc_2019_rs %in% c(90, 95) 
otherUndeveloped_2019 <- lc_2019_rs %in% c(52, 71, 31)
water_2019 <- lc_2019_rs == 11
```


### 5.2.2. Aggregating Rasters to the Fishnet

We have two steps to prepare our rasters for batch aggregation. You should follow this workflow as-is when you build your urban growth model for your assignment.

First, we apply `names` to our rasters. When we aggregate all the rasters, the `names` ensure that when the raster is integrated with the fishnet, the column reflects the appropriate raster.

Second, we generate a list of land cover rasters for 2011, which we call `rasterList_2011`, and a list for 2019, which we call `rasterList_2019`. These are lists of objects that we can feed to `aggregateRaster` to instruct it which data to aggregate to fishnet.

```{r, warning = FALSE, message = FALSE}

names(developed_2011) <- "developed_2011"
names(forest_2011) <- "forest_2011"
names(farm_2011) <- "farm_2011"
names(wetlands_2011) <- "wetlands_2011"
names(otherUndeveloped_2011) <- "otherUndeveloped_2011"
names(water_2011) <- "water_2011"

names(developed_2019) <- "developed_2019"
names(forest_2019) <- "forest_2019"
names(farm_2019) <- "farm_2019"
names(wetlands_2019) <- "wetlands_2019"
names(otherUndeveloped_2019) <- "otherUndeveloped_2019"
names(water_2019) <- "water_2019"
```


```{r, warning = FALSE, message = FALSE}
rasterList_2011 <- c(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011)

rasterList_2019 <- c(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019)
```

Lastly, we send our lists (e.g. `rasterList_2011`) and our basic fishnet (`houstonMSA_fishnet`) to `aggregateRaster`, and create new fishnets called `lcRasters_2011` and `lc_Rasters_2019` that have rows for each fishnet cell, and columns with 0/1 values for all of our land cover types.

What this will allow us to do, in the end, is use land cover types as dummy variables in our model - in other words, we can make predictions of a cell's likelihood to develop based on a hypothesized association between a cell's land cover in `t1` (e.g. forest, wetlands etc.,) and it's developability.


```{r, warning = FALSE, message = FALSE}
lcRasters_2011 <-
  aggregateRaster(rasterList_2011, 
                  houstonMSA_fishnet) %>%
  dplyr::select(developed_2011,
                   forest_2011,
                   farm_2011,
                   wetlands_2011,
                   otherUndeveloped_2011,
                   water_2011) %>%
  mutate_if(is.numeric,as.factor)

lcRasters_2019 <-
  aggregateRaster(rasterList_2019, 
                  houstonMSA_fishnet) %>%
  dplyr::select(developed_2019,
                   forest_2019,
                   farm_2019,
                   wetlands_2019,
                   otherUndeveloped_2019,
                   water_2019) %>%
  mutate_if(is.numeric,as.factor)

```


We can now plot the aggregated rasters in a facetted fashion and examine them.

Which type of land cover status do you think would be good predictors of development? For example, it would be a reasonable hypothesis that wetlands - due to either their preservation status or their difficulty to excavate - would be associated with a low likelihood of development.

```{r, message = FALSE, warning = FALSE}
lcRasters_2011 %>%
  st_centroid() %>%
 gather(key = "variable", value = "value", developed_2011:water_2011) %>% 
  mutate(X = xyC(.)$x,
         Y = xyC(.)$y) %>%
  ggplot() +
    geom_sf(data=houstonMSA) +
    geom_point(aes(X,Y, colour=as.factor(value))) +
    facet_wrap(~variable) +
    scale_colour_manual(values = palette2,
                        labels=c("Other","Land Cover"),
                        name = "") +
    labs(title = "Land Cover Types, 2011",
         subtitle = "As fishnet centroids") +
   theme_void()
```

# 6. Wrangle Census Data and Join To the Fishnet

Population density is obviously a critical demand-side component of predicting `Development_Demand`. Census data for both 2011 and 2019 can be downloaded quickly using the `tidycensus` package. As illustrated below, these data are downloaded at a census tract geography and thus, an approach is needed to reconcile tracts and fishnet geometries. This is accomplished using a technique called areal weighted interpolation.

Keep in mind, when you are choosing your time period of interest for your study, that the ACS came into being in the mid 2000s - so models based on data further back might need to use decennial census data sets and different `tidycensus` API calls.

One useful variable that we do not explore in this exercise is population change - it stands to reason that an undeveloped grid cell in or proximate to an area with accelerating growth (e.g. a density higher than that of 5 or 10 years previous) would be likely to convert.

Recall, you will need a census API key to download the census data which must be input with `census_api_key`. 

## 6.1. Downloading Census Data via API

Load your census API key by pasting it into the code block below with the function `census_api_key`

```{r load_key_hide, warning= FALSE, include=FALSE}
census_key <- read.table("~/GitHub/census_key.txt", quote="\"", comment.char="")
census_api_key(census_key[1] %>% as.character(), overwrite = TRUE)
```

```{r load_key, warning = FALSE, eval = FALSE}
census_api_key("YOUR KEY GOES HERE", overwrite = TRUE)
```

First data we pull population data for `t1` and reproject it to the same crs as our study fishnet - `houstonMSA_fishnet`.

In your growth model - you can pull more demographic and economic variables here by adding them to the `acs_vars` list of data that you'd like to call from the Census.

Note that you need to specify your study area in the API call - here I specify the state as `48` (The two-letter abbreviation `TX` would also work), and I specify the county with a list of the counties in my study area.

This is a nice dplyr chain that does the following:

- DL the data, 
- `select` only the Estimates, NAME, and GEOID column (e.g. removing the margin of error columns)
- `rename` my variables to be intelligible, 
- `st_transform` the data to the `houstonMSA_fishnet` crs
- `st_buffer` the geometries a teeny tiny bit (by negative 1 map unit) - this is done because sometimes (only sometimes) you have twisted and broken geometries, and this fixes it. If you don't do this, when we do aerially weighted interpolation, you might get an error.

```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2011 as the year, download data data for the Houston MSA counties listed.
houstonPop11 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2011,
                        state = 48, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Harris COunty","San Jacinto","Montgomery","Liberty","Waller",
                         "Austin","Chambers","Fort Bend","Brazoria","Galveston")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2011 = B02001_001E) %>%
                st_transform(st_crs(houstonMSA_fishnet)) %>%
                st_buffer(-1)
```

We do this once more for `t2`, in this case, 2019.


```{r, warning = FALSE, message = FALSE, results = "hide"}
# Specify which variable(s) you would like to grab. Here, only one (Total Population) is listed, but you could add more to the call.
acs_vars <- c("B02001_001E")

# Using "tract" as the geography and 2019 as the year, download data data for the Houston MSA counties listed.
houstonPop19 <- get_acs(geography = "tract", 
                        variables = acs_vars, 
                        year = 2019,
                        state = 48, 
                        geometry = TRUE, 
                        output = "wide",
                        county=c("Harris COunty","San Jacinto","Montgomery","Liberty","Waller",
                         "Austin","Chambers","Fort Bend","Brazoria","Galveston")) %>%
                dplyr::select (GEOID, NAME, acs_vars) %>%
                rename(pop_2019 = B02001_001E) %>%
                st_transform(st_crs(houstonMSA_fishnet)) %>%
                st_buffer(-1)
```

Then we can plot our data and see the spatial arrangement of population in our study area in t1 and t2.

One thing worth noting here is that the quantile symbology that we apply probably changes from t1 to t2 - so you can see that, on average, there was densification across the metro area on this time interval - the quantile ranges shift upwards.

<div class="superbigimage">
```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data = houstonPop11, aes(fill=factor(ntile(pop_2011,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(houstonPop11,"pop_2011"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Houston MSA: 2011") +
  theme_void(),

ggplot() +
  geom_sf(data = houstonPop19, aes(fill=factor(ntile(pop_2019,5))), colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=quintileBreaks(houstonPop19,"pop_2019"),
                   name="Quintile\nBreaks") +
  labs(title="Population, Houston MSA: 2019") +
  theme_void(), ncol=2)
```

## 6.2. Aggregate Census Data with Aerially Weighted Interpolation

AWI is a cool spatial analysis tool, you may not have used it before - we can use it to reconcile tract boundaries and fishnet grid cells, and apportion population from the tracts to the cells. Here is how Prof. Steif described AWI -

*Areal weighted interpolation is a really strong spatial analysis skill to have. You can do this in ArcGIS but there is not automated approach.*

*A spatial join would be inappropriate as it would assign the same population value from one tract to the many intersecting grid cells. Instead, the area weighted interpolation function ... assigns a proportion of a tract’s population to a grid cell weighted by the proportion of the tract that intersects the grid cell. This works best of course, when we assume that the tract population is uniformly distributed across the tract. This is typically not a great assumption. However, it is a reasonable one here particularly given population is a feature in a regression and not an outcome that needs to be measured with significant precision. *

*Ken Steif, 2019*

The code chunks below perform AWI for our `t1` and `t2` census data using the `st_interpolate_aw` function. This process might take a bit of time to run. The output is a new fishnet for each year that consists of a population estimate and a geometry. It is important to note that your tracts and your fishnet might have points where they fail to overlap, and you might lose the fidelity of your analysis by dropping fishnet cells. This workflow is 

Here is a breakdown of the workflow so that you can adapt it to use other data sets in your analysis.

- `st_interpolate_aw` takes an sf object (`houstonPop11`), a column name from that sf object (`["pop_2011"]`) and our fishnet `houstonMSA_fishnet`. Leave the argument `extensive=TRUE` as-is.

- Knowing that our result might have a differing number of cells from the fishnet, we convert this object to centroids (`st_centroid`) and spatially join it to our fishnet using `st_join` - a join initiated from the `houstonMSA_fishnet` object. This finds the points where our AWI results intersect the fishnet cells, and keeps any cells where there is no intersection,

- Lastly, we perform some data cleaning, where we replace NA values (`replace_na`) with zeros, and `select` only our `pop_2011` column to remain.

Keep an eye on how many cells your fishnets have throughout the process - if you are adding or dropping cells, something is going wrong.

If you get warning messages about "constant or uniform" geometries - this is just the `sf` package reminding you to be on your p's and q's about projections - it won't protect you if your projections don't match!

```{r, warning = FALSE, message = FALSE}
fishnetPopulation11 <-
  st_interpolate_aw(houstonPop11["pop_2011"], 
                    houstonMSA_fishnet, 
                    extensive=TRUE) %>%
  st_centroid() %>%
  st_join(houstonMSA_fishnet, ., join = st_intersects) %>%
  mutate(pop_2011 = replace_na(pop_2011,0)) %>%
  dplyr::select(pop_2011)
```

We repeat the process for our `t2` data.


```{r}
fishnetPopulation19 <-
  st_interpolate_aw(houstonPop19["pop_2019"],
                    houstonMSA_fishnet, 
                    extensive=TRUE) %>%
  st_centroid() %>%
  st_join(houstonMSA_fishnet, ., join = st_intersects) %>%
  mutate(pop_2019 = replace_na(pop_2019,0)) %>%
  dplyr::select(pop_2019)
```

How do these data look when we map them out and compare them to the census vectors? We can see that the modifiable aerial unit issue - where smaller tracts are higher density because of the logic of tract-drawing - no longer distorts the picture - we see increased density in the center, where we expect it.

<div class="superbigimage">
```{r, warning = FALSE, message = FALSE, fig.height = 8, fig.width= 11}
grid.arrange(
ggplot() +
  geom_sf(data=houstonPop11, aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                    labels=substr(quintileBreaks(houstonPop11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Houston MSA: 2011",
       subtitle="Represented as tracts; Boundaries omitted") +
  theme_void(),

  ggplot() +
  geom_sf(data=fishnetPopulation11, 
         aes(fill=factor(ntile(pop_2011,5))),colour=NA) +
  scale_fill_manual(values = palette5,
                   labels=substr(quintileBreaks(fishnetPopulation11,"pop_2011"),1,4),
                   name="Quintile\nBreaks") +
  labs(title="Population, Houston MSA: 2011",
       subtitle="Represented as fishnet gridcells; Boundaries omitted") +
  theme_void(), ncol=2)
```
</div>

# 7. Transportation And Infrastructure

Accessibility is a key determinant of development potential. The hypothesis here is that the closer a cell is to highway infrastructure, the more likely it is to develop. Accessibility features are engineered by measuring distance from each grid cell to its nearest highway. 

This example only uses highway distance, but one could easily add spatial information about transit lines, bus networks or other relevant infrastructure as predictors. In urban growth modeling, one might model scenarios where new infrastructure is developed, such as a new highway or a new transit line.

To manipulate this workflow and include new infrastructure, you can use ArcGIS or other software to draw a new highway, and then bring that shape into a modeling workflow in this section and calculate new (hypothetical) distance-to-highway variables for each cell in `t2` to use as a basis for forecasting an alternate future `t3` where some cells now have a different level of access to transportation.

## 7.1. Download Highways

First highway vectors (`primary_roads`) are downloaded from the `Tigris` package - we project the data and subset it to the study area using `st_intersection`.

```{r, warning = FALSE, message = FALSE, results = "hide"}
houstonHighways <-
  tigris::primary_secondary_roads(state = "TX") %>%
  st_transform(st_crs(houstonMSA)) %>%
  st_intersection(houstonMSA) %>%
  st_transform(st_crs(fishnet))
```

Let's make a map and examine the spatial relationship between highways and development.

```{r plot_highway, warning = FALSE, message= FALSE}
ggplot() +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2],colour=development_change),size=1.5) +
  geom_sf(data=houstonHighways) +
  scale_colour_manual(values = palette2,
                      labels=c("No Change","New Development")) +
  labs(title = "New Development and Highways",
       subtitle = "As fishnet centroids") +
  theme_void()
```

## 7.2. Calcuate Distance to Highways

We can create a new fishnet variable called `distance_highways_2011` by turning our `houstonMSA_fishnet` into centroid points and calculating the distance from each cell centroid to the nearest highway segment. This is done in a `mutate` command that utilizes the `st_distance` function to run this distance calculation for each observation.

We join these observations back to a fishnet, and the result is a grid called `highwayPoints_fishnet` with a column called `distance_highways_2011`.

An important note:

This analysis operates on the *assumption* that the transporation network in `t2` (2019) is the same as in `t1` - so we create a fishnet representing `t2` that is the same - we merely copy it.

*If you are trying to input a different set of transportation infrastructure as the basis for your future projection for t3 - you will want to import this new network info at this step and create an alternative `highwayPoints_fishnet_2019` (or whatever name represents t2 in your analysis). This information will be an important variable in your model that you use to project future development change.*

```{r, warning = FALSE, message = FALSE}
highwayPoints_fishnet_2011 <- houstonMSA_fishnet %>%
  st_centroid() %>%
  mutate(distance_highways_2011 = as.numeric(st_distance(., st_union(houstonHighways) %>% 
                                                      st_transform(st_crs(houstonMSA_fishnet))))) %>%
  as.data.frame() %>% 
  dplyr::select(-geometry) %>% 
  left_join(houstonMSA_fishnet, .) %>% 
  st_as_sf()

highwayPoints_fishnet_2019 <- highwayPoints_fishnet_2011 %>%
  rename(distance_highways_2019 = distance_highways_2011)

```

```{r, warning = FALSE, message = FALSE}

ggplot() +
  geom_sf(data=houstonMSA %>% st_transform(st_crs(highwayPoints_fishnet_2011))) +
  geom_point(data=highwayPoints_fishnet_2011, aes(x=xyC(highwayPoints_fishnet_2011)[,1], 
                                             y=xyC(highwayPoints_fishnet_2011)[,2], 
                 colour=factor(ntile(distance_highways_2011,5))),size=1.5) +
  scale_colour_manual(values = palette5,
                      labels=substr(quintileBreaks(highwayPoints_fishnet_2011,"distance_highways_2011"),1,8),
                      name="Quintile\nBreaks") +
  geom_sf(data=houstonHighways, colour = "red") +
  labs(title = "Distance to Highways (m)",
       subtitle = "As fishnet centroids; Highways visualized in red") +
  theme_void()
```

# 8. Calculate spatial lag of development

As we read in Prof. Steif's introductory essay, the core of our modelling approach is the harnassing of spatial endogenaeity - we hypothesize that the likelihood of a cell to develop is (in part) a function of its proximity to existing development. How do we "parameterize" the adjacency or access of each cell relative to existing development? We use "spatial lag" variables like k-nearest-neighbors (knn).

In this example, we calculate the average distance to each grid cell's 2 nearest developed grid cells in year `t1`. We do this using a custom function that we loaded earlier called `nn_function` to create new variables in our `fishnet` grid for `lagDevelopment_2011` and `lagDevelopment_2019`. 

The `nn_function` takes three arguments - two data frames with lists of x-y coordinates, and a k.

- The first parameter specifies an sf object whose coordinates we want to `measureFrom`, in this case, all `fishnet` centroids. Since the function requires a data frame with x-y coordinates, we take our fishnet, convert it to centroids (`st_centroid`), extract the coordinates to a new data frame (`st_coordinates`, `as.data.frame()`)

- The second, indicates the sf point data we wish to `measureTo`, in this case, the fishnet centroids that were developed in `t1`. We `filter` our `lcRasters_2011` to include only our developed cells, and then we follow the steps we took for our fishnet data, converting to centroid, and extracting coordinates to a new data frame.

- The last parameter is our k - how many neighbors to which we'd like to measure average distance. Why `k=2`? As `k` fluctuates, so does the hypothesized scale of accessibility. One can test the effect of different k parameters on model goodness of fit. A more sophisticated model would hypothesize that this scale can vary significantly from city to suburb to rural town.

```{r, warning = FALSE, message = FALSE}
fishnet$lagDevelopment_2011 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2011 %>%
                  filter(developed_2011 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)

fishnet$lagDevelopment_2019 <-
    nn_function(fishnet %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                lcRasters_2019 %>%
                  filter(developed_2019 == 1) %>%
                  st_centroid() %>%
                  st_coordinates() %>%
                  as.data.frame(),
                2)
```

```{r, warning = FALSE, message = FALSE}
ggplot() +
  geom_sf(data=houstonMSA %>% st_transform(st_crs(fishnet))) +
  geom_point(data=fishnet, 
             aes(x=xyC(fishnet)[,1], y=xyC(fishnet)[,2], 
                 colour=factor(ntile(lagDevelopment_2011,5))), size=1.5) +
  scale_colour_manual(values = palette5,
                     labels=substr(quintileBreaks(fishnet,"lagDevelopment_2011"),1,7),
                     name="Quintile\nBreaks") +
  labs(title = "Spatial Lag to 2011 Development, m",
       subtitle = "As fishnet centroids") +
  theme_void()
```


# 9. Political Boundaries

We are going to add information about county boundaries to our data set. This serves two important purposes. First, it might be useful to include municipality as a fixed effect in your models to add information about regulatory "culture" or desirability of different areas. Using a geographic fixed effect adds the "average effect" of a particular area on the dependent variable - commonly this is used as a variable in housing price models to incorporate place-based price signals from things like school district or neighborhood reputation. Second, we can use this information to evaluate and analyze our model outputs - seeing where development is expected to take place and evaluate our errors across space.

We can get data on counties from the US Census Bureau via the `tigris` package, transform it to the CRS of our study area, and then `filter` our county data to contain only the relevant shapes for our study area.

```{r, warning = FALSE, message = FALSE, results = "hide"}
options(tigris_class = "sf")

counties <- counties(state = "TX") %>%
  st_as_sf() %>%
  st_transform(st_crs(houstonMSA_fishnet))

studyAreaCounties <- counties %>%
  filter(NAME %in% c("Harris","San Jacinto","Montgomery","Liberty","Waller",
                         "Austin","Chambers","Fort Bend","Brazoria","Galveston"))

```

We can spatially join the `studyAreaCounties` object to our fishnet and create a `countyFishnet` where each cell is imparted with the name of the county it belongs to.

```{r}
countyFishnet <- houstonMSA_fishnet %>%
  st_join(., studyAreaCounties %>%
            dplyr::select(NAME)) %>%
  as.data.frame() %>% 
  dplyr::select(uniqueID, NAME) %>% 
  left_join(houstonMSA_fishnet, .) %>% 
  st_as_sf() %>%
  group_by(uniqueID) %>%
  slice(1) %>%
  ungroup() %>%
  arrange(as.numeric(uniqueID))
  
```

# 10. Creating Our Data Set for Modeling

We've done a substantial amount of data wrangling and transformation, and now we can put all of our fishnets together into a data set that we can explore (as part of our feature engineering) and ultimately model. We will need to prepare data sets for `t1` (used to train a model that can predict `development_change` between `t1` and `t2`), and for `t2` (which uses the same variables to forecast change in `t3`).

If there are additional elements that you want to add to your modeling, this is the step where you can put them together. If you are doing your data wrangling using ArcGIS, this is a point at which you'd want to bring your data in to create data sets that are similar to those we create below:

Let's take stock of what we have:

For t1

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

For t2

- fishnet
- highwayPoints_fishnet_2011
- fishnetPopulation11
- lcRasters_2011
- countyFishnet

Each of these fishnets should have the same number of cells (in this analysis, that's 32816), and given that they were created in the same manner, they will be indexed identically, and can be appended to one another using `cbind`.  Using `select`, we keep only our relevant independent variables (lc type, transport distance, county name, lag to development etc.,), our dependent variable (development change), our uniqueID and our geometry.

In the last two step of our data binding process below we use a function called `rename_with` to remove the string indicating the year from our column names. Why do we do this? It's because our trained model (built using `t1` data) will columns of the same name and data type for `t2` to make predictions. The training data is all it "knows" - so we must prepare similarly formatted data to deploy it.

```{r}


dat_2011 <- 
  cbind( fishnet, highwayPoints_fishnet_2011, fishnetPopulation11, lcRasters_2011, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2011, distance_highways_2011, pop_2011,  
                developed_2011, forest_2011, farm_2011, wetlands_2011, otherUndeveloped_2011, water_2011,
                NAME, geometry) %>%
  filter(water_2011 == 0) %>%
  rename_with(~ str_remove(.x, "_2011"))

dat_2019 <- 
  cbind( fishnet, highwayPoints_fishnet_2019, fishnetPopulation19, lcRasters_2019, countyFishnet) %>%
  as.data.frame() %>%
  dplyr::select(uniqueID, development_change, lagDevelopment_2019, distance_highways_2019, pop_2019,  
                developed_2019, forest_2019, farm_2019, wetlands_2019, otherUndeveloped_2019, water_2019,
                NAME, geometry) %>%
  filter(water_2019 == 0)  %>%
  rename_with(~ str_remove(.x, "_2019"))
  

```

# 11. Feature Exploration

In this section we explore the extent to our possible predictors are associated with development change. If the goal was to predict a continuous variable, scatterplots and correlation coefficients make this process straightforward and relatively easy to explain to a non-technical decison maker.

In this case however, the dependent variable is a binary outcome - either a grid cell was developed between `t1` and `t2` or it wasn’t. In this case, the relevant question is whether for a given feature, there is a notable significant difference in its central tendancies or distribution between areas that changed and areas that did not. These differences are explored in a set of plots below. For models with lots of features, these plots could be complimented by a series of statistical tests examining the difference between a predictor's mean for `1` and `0` cases.

Here, we `select` our continuous varialbes and our dependent variable, convert our to long form using `gather` created facetted bar plots. Note that `geom_bar` calculates the `mean` has a `fun.y` argument where we can calculate the mean value for these variables inside our plotting function. 

How do you interpret each of these plots? Try the following framework - "On average, a cell that converted from undeveloped to developed had was __________ compared to a cell that didn't convert."

Do you think these are going to be good predictors of `development_change`? Why?

```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(distance_highways,lagDevelopment,development_change, pop) %>%
  gather(Variable, Value, -development_change) %>%
  ggplot(., aes(development_change, Value, fill=development_change)) + 
    geom_bar(position = "dodge", stat = "summary", fun.y = "mean") +
    facet_wrap(~Variable, scales = "free") +
    scale_fill_manual(values = palette2,
                      labels=c("No Change","New Development"),
                      name="Mean Value") +
    labs(title="New Development as a Function of Continuous Variables") +
    theme_minimal() 
```

It's also useful to look at the patterns of change between different land cover types. What proportion of each undeveloped land cover type converted to developed during the t1-t2 interval?


```{r, warning = FALSE, message = FALSE}
dat_2011 %>%
  dplyr::select(development_change, forest, farm, wetlands, otherUndeveloped) %>%
  gather(key = "Land_Cover_Type", Value, -development_change) %>%
     group_by(development_change, Land_Cover_Type) %>%
     summarize(n = sum(as.numeric(Value))) %>%
     ungroup() %>%
    mutate(Conversion_Rate = paste0(round(100 * n/sum(n), 2), "%")) %>%
    filter(development_change == 1) %>%
  dplyr::select(Land_Cover_Type,Conversion_Rate) %>%
  kable() %>% kable_styling(full_width = F)
```

# 12. Modeling

We are going to train and test our models using a workflow based on the one we learned in the ["Introduction to Applied Predictive Modeling" text](https://mafichman.github.io/applied_predictive_modeling/#1_Introduction), where we used Binary Logistic Regression to estimate morbidity in plants.

## 12.1. Splitting our data

First, `dat` is split into 50% training and test sets. Since we have relatively few 1's in our data, a generous train/test split helps keeps a fair number of 1's in the training set. Note how we specify `dat_2011$otherUndeveloped` as needing to be sorted into our training set - there are relatively few of these, and we want to make sure our model has "seen" all levels of our fixed effects in training.

```{r, warning = FALSE, message = FALSE}
set.seed(3456)
trainIndex <- 
  createDataPartition(dat_2011$otherUndeveloped, p = .50,
                                  list = FALSE,
                                  times = 1)
datTrain <- dat_2011[ trainIndex,]
datTest  <- dat_2011[-trainIndex,]

```

## 12.2. Specifying models

We estimate six separate `glm` models - adding new variables for each.

- `Model1` includes only previous land cover types. 

- `Model2` adds the `lagDevelopment`. 

- `Model3` adds population

- `Model4` adds a fixed effect for county (`NAME`)

- `Model5` adds the distance to highway.

- `Model6` is a modification of `Model5` which *interacts* distance to highway and development lag. The hypothesis here is that these two variables are related - the effect of one depends on the other. e.g. Distance to nearest development depends in part on access to transportation. Notice that the effect of both variables is significant in this specification, but both are not significant in `Model5`.

Which model is best? The one that performs the most usefully in our forecasting use case! As it turns out, although county is not significant in our models, it proves useful when we examine our outcomes.

```{r, warning = FALSE, message = FALSE}
Model1 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped, 
              family="binomial"(link="logit"), data = datTrain)

Model2 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment, 
              family="binomial"(link="logit"), data = datTrain)
              
Model3 <- glm(development_change ~ wetlands + forest  + farm +
                otherUndeveloped + lagDevelopment + pop,
              family="binomial"(link="logit"), data = datTrain)          
              
Model4 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model5 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + lagDevelopment + pop + distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 

Model6 <- glm(development_change ~ wetlands + forest  + farm + 
                otherUndeveloped + pop + lagDevelopment * distance_highways + NAME, 
              family="binomial"(link="logit"), data = datTrain) 
```


Print the summary objects for your models, and choose one to use as your model for the validation and prediction procedure to follow.


```{r}

summary(Model6)

```
You can compare the AIC indicators for the models to get a sense of their general capacity to explain variation in your dependent variable. The lower the AIC, the better the goodness of fit - remember, this is NOT the same as model "quality" when it comes to prediction!

```{r}
data.frame(
  Model = c("Model1", "Model2", "Model3", "Model4", "Model5", "Model6"),
  AIC = c(Model1$aic, Model2$aic, Model3$aic, Model4$aic, Model5$aic, Model6$aic)
) %>%
  ggplot()+
  geom_bar(aes(x = Model, y = AIC), stat = "identity")+
  theme_minimal()

```

## 12.3. Validating our Model Using the Test Set

We create a new data frame `testSetProbs` that consists of our `class` (e.g. development_change as a 1 or 0), and `probs` - which is the prediction for each observation in our test set using `Model6`. The `type` parameter is set to `response`, which means our `probs` are measures of estimated probability from 0-1.

```{r, warning = FALSE, message = FALSE}
testSetProbs <- 
  data.frame(class = datTest$development_change,
             probs = predict(Model6, datTest, type="response")) 
```

This density plot is a key tool to figure out where we will set our threshold for classifying predicted probabilities as 1's or 0's (eg Predicted to Develop or Predicted Not To Develop).

Take a close look at this plot and see if you have some ideas about what threshold would separate out our 1's from our 0's most effectively with a minimum of error.

```{r}  
ggplot(testSetProbs, aes(probs)) +
  geom_density(aes(fill=class), alpha=0.5) +
  scale_fill_manual(values = palette2,
                    labels=c("No Change","New Development")) +
  labs(title = "Histogram of test set predicted probabilities",
       x="Predicted Probabilities",y="Density") +
  theme_minimal()
```


### 12.3.1. Confusion matrix

Let's say that 0.05 is a good cutoff value - above that value, we predict that cell is a `1`, and below it, a `0`.

Let's build a confusion matrix to see how accurate this model is overall, and what our error rates are.

It helps to write out in plain language what our four outcomes are:

- True Positive: We predicted a cell would convert from undeveloped to developed and it *did* convert.

- False Positive: We predicted a cell would convert from undeveloped to developed and it *did not* convert.

- True Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did not* convert.

- False Negative: We predicted a cell would *not* convert from undeveloped to developed and it *did* convert.

If you want to adjust model performance, or use a different model, you can change the model type in the previous step and/or change the threshold in this step (see the first line of code) and compare the performance.

```{r}
testSetProbs$predClass  = ifelse(testSetProbs$probs > .05 ,1,0)

caret::confusionMatrix(reference = as.factor(testSetProbs$class), 
                       data = as.factor(testSetProbs$predClass), 
                       positive = "1")

```

### 12.3.2. ROC Curve

One more indicator of model performance - the ROC curve. Do we have a "healthy" looking curve? Above the y=x line, not a "hard elbow" shape, and relatively convex?

In this case, we certainly do.

```{r roc_curve, message = FALSE, warning = FALSE}

ggplot(testSetProbs, aes(d = as.numeric(class), m = probs)) + 
  geom_roc(n.cuts = 50, labels = FALSE) + 
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  theme_minimal()
```

## 12.4. Analyzing Error

Now that we have our model, we want to assess our errors across our entire *t2* data set for a few different thresholds and do some "generalizability" tests. 

We should think critically about model thresholds. What does a planner want to optimize for in this situation? Are we mainly concerned with correctly predicting development (True Positives)? What if that means we erroneously predict some areas that *did change* as *no change* (False Negatives)? 

Let's say, for our purposes, that we want to be generous about predicting change, so we have a more general spatial understanding of where change *might* take place, and not accidentally overlook an area where we miss the trend because we were too conservative. You are welcome to take a different approach to this question when you do such a model for yourself - this is a context-dependent choice.

### 12.4.1. Setting a Threshold

Let's compare two different thresholds - 5% and 17%. In plain English, this means that if a predicted probability is greater than 0.05, under the 5% regime, we say "this is classified as a 1".  At a 5% threshold we correctly predict a higher rate of new development areas (True Positive Rate), but incorrectly predicts more no change areas (True Negative Rate).  Conversely, the 17% threshold has a lower True Positive rate and but a far higher True Negative rate. Because most of our cells do not change from *t1* to *t2*, this leads to higher accuracy.

What you think is better here as a threshold? This is a matter of your expertise.

The mechanics of the code below are as follows. 

- We create a new sf object called `dat_2011_preds` starting with `dat_2011` (our whole `t2` data set). 

- We pipe in a mutate command, where we first use the `predict` function to generate probability estimates for each row in `dat_2011` using our model of choice, `Model6`. These estimates are stored in a column called `probs`.

- In a second `mutate` command, we create two new columns classifying our predictions based on `ifelse` statements. e.g. "If `probs` is greater or equal to 0.05, set `Threshold_5_Pct` equal to 1, else set it equal to 0"

- We subsequently use mutate statements to create columns that reflect the accuracy of these classifications using `case_when` statements - which is basically an elaborate `ifelse` statement based on a bunch of conditions - e.g. "If the `Threshold_5_Pct` is a 1, and `development_change` (e.g. the observed), set `confResult_05` to "True_Negative", etc.,

```{r, warning = FALSE, message = FALSE}
dat_2011_preds <-         
  dat_2011 %>%
    mutate(probs = predict(Model6, dat_2011, type="response")) %>%
   mutate(Threshold_5_Pct = as.factor(ifelse(probs >= 0.05 ,1,0)),
           Threshold_17_Pct =  as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  mutate(confResult_05 =case_when(Threshold_5_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_5_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_5_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_5_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  mutate(confResult_17 =case_when(Threshold_17_Pct == 0 & development_change == 0 ~ "True_Negative",
                              Threshold_17_Pct == 1 & development_change==1 ~ "True_Positive",
                              Threshold_17_Pct == 0 & development_change==1 ~ "False_Negative",
                              Threshold_17_Pct == 1 & development_change ==0 ~ "False_Positive")) %>%
  st_as_sf()
```

Now we can summarize our results by threshold *and* by any other category in our data to see how these thresholds affect our errors.

Let's cross-tabulate our errors by threshold and by County. Do we think there are combinations of county and threshold that are sub-optimal?

```{r}

# Summarize by county and model type
dat_2011_preds %>%
  as.data.frame() %>%
  dplyr::select(confResult_05, confResult_17, NAME) %>%
  pivot_longer(cols = starts_with("confResult"), names_to = "Model_Type", values_to = "Confusion_Result") %>%
  group_by(NAME, Model_Type, Confusion_Result) %>%
  tally() %>%
  pivot_wider(names_from = Confusion_Result, values_from = n, values_fill = 0) %>% # Reshape to wide format
  mutate(TN_Rate_Specificity = 100*( True_Negative/(True_Negative+False_Positive)),
         TP_Rate_Sensitivity = 100*( True_Positive/(True_Positive + False_Negative))) %>%
  dplyr::select(NAME, Model_Type, TN_Rate_Specificity, TP_Rate_Sensitivity) %>%
  kable() %>%
  kable_styling()



```

We can also map our errors and see how our threshold choices manifest themselves across space in our study area.

Notice how the spatial pattern of True Positives for both thresholds is relatively consistent, but the 5% threshold misses most the study area with respect to True Negatives.

<div class="superbigimage">
```{r, warning = FALSE, message= FALSE, fig.height = 6, fig.width= 8}
ggplot() +
  geom_sf(data= dat_2011_preds %>%
            st_centroid() %>%
               dplyr::select(confResult_05, confResult_17, geometry) %>%
               gather(key = "Variable", value = "Value", -geometry), 
             aes(colour=Value)) +
  facet_wrap(~Variable) +
  scale_colour_manual(values = c("red", "yellow", "blue", "grey"), labels=c("False Negative","False Positive", "True Negative", "True Positive"),
                      name="") +
  labs(title="Development Predictions - By Threshold") + 
  theme_void()
```
</div>


# 13. Making A Forecast

OK, we are ready to use our model to forecast future development. Let's use our model `Model6` with a threshold of 0.17 - this will allow us to make a forecast that isn't quite as conservative and minimizes false negatives.

Let's create an sf object called `dat_2027_preds` by feeding our `t2` data to `Model6`.

*To create a forecast that incorporates new transportation infrastructure - your dat_2019 should have distance to transportation that was calculated using a shape that has a new highway as part of its geometry.*

```{r}
dat_2027_preds <- dat_2019 %>%
    mutate(probs = predict(Model6, dat_2019, type="response") ,
           Prediction = as.factor(ifelse(probs >= 0.17 ,1,0))) %>%
  st_as_sf()
  

```


Now lets map it - where are the cells that are classified as likely to develop by 2027?


```{r, warning = FALSE, message = FALSE, fig.height= 8, fig.width= 8 }
ggplot(data=dat_2027_preds) +
  geom_point(aes(x=xyC(dat_2027_preds)[,1], 
                 y=xyC(dat_2027_preds)[,2], colour = Prediction)) +
  geom_sf(data = studyAreaCounties, fill = "transparent")+
  labs(title="Development Predictions, 2027") + 
  theme_void()
```

# 14. Assess Impact

Once we have a forecast, it's the planner's job to think about how to react to the likely impacts of this anticipated demand with a strategy for allocating development through the use of planning - incentives, regulations etc.,

There is a lot of exploratory analysis you might want to do with your model outputs. For example, you could use the `mapview` package to make a quick interactive map and explore communities in your study area in more detail. You can spatially cross-reference your prediction with data found in supplementary data sets, like parks, parcels and other land use data.

## 14.1. Impact Assessment

How much land is anticipated to convert by 2027? How much in total? How much by county? How much sensitive land is expected to convert?

Overall land forecasted land conversion can be converted from cells to raw area by multiplying by the cell resolution - `res(lc_2019_rs)[1]`

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

County-by-county forecast:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  group_by(NAME) %>%
  tally() %>%
  rename(total_cells = n) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

Forecast by `t2` land cover type:

```{r}
dat_2027_preds %>%
  as.data.frame() %>%
  filter(Prediction == 1) %>%
  dplyr::select(farm, otherUndeveloped, forest, wetlands, NAME) %>%
  gather(-NAME, key = "Variable", value = "Value") %>%
  group_by(NAME, Variable) %>%
  summarize(total_cells = sum(as.numeric(Value))) %>%
  mutate(total_area_m = total_cells * res(lc_2019_rs)[1],
         total_km2 = total_area_m/1000000,
         total_mi2 = total_km2*0.386102) %>%
  kable() %>%
  kable_styling ()

```

# 15. Next Steps - Towards an Allocation Strategy

What do you do after you have a geographically specific forecast of likely development? You can integrate it with population projection information to see if forecasted growth areas seem adequate to accommodate future growth. You can dive in on areas where sensitive lands are at risk and determine if upzoning in other areas of nearby development might accommodate that population. You can compare future scenarios for fragmentation effects. There are myriad possibilities.


